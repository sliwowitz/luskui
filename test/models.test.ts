import test from "node:test";
import assert from "node:assert/strict";
import path from "node:path";
import { fileURLToPath, pathToFileURL } from "node:url";
import { randomUUID } from "node:crypto";
import { FALLBACK_MODELS } from "../lib/config.js";

const __filename = fileURLToPath(import.meta.url);
const __dirname = path.dirname(__filename);
const modelsModuleHref = pathToFileURL(
  path.join(__dirname, "..", "lib", "backends", "codex", "models.js")
).href;

const originalFetch = global.fetch;
const trackedEnvKeys = ["OPENAI_API_KEY", "CODEXUI_MODEL_CACHE_MS"] as const;
type EnvKey = (typeof trackedEnvKeys)[number];
const originalEnv: Record<EnvKey, string | undefined> = Object.fromEntries(
  trackedEnvKeys.map((key) => [key, process.env[key]])
) as Record<EnvKey, string | undefined>;

test.afterEach(() => {
  for (const key of trackedEnvKeys) {
    if (originalEnv[key] === undefined) {
      delete process.env[key];
    } else {
      process.env[key] = originalEnv[key];
    }
  }
  global.fetch = originalFetch;
});

type FetchImpl = typeof fetch | null | undefined;
type ModelsModule = typeof import("../lib/backends/codex/models.js");

async function loadModelsModule({
  apiKey,
  fetchImpl,
  cacheMs
}: Record<string, unknown> = {}): Promise<ModelsModule> {
  applyEnvOverride("OPENAI_API_KEY", apiKey as string | null | undefined);
  applyEnvOverride("CODEXUI_MODEL_CACHE_MS", cacheMs as string | number | null | undefined);
  if (fetchImpl === undefined) {
    global.fetch = originalFetch as typeof fetch;
  } else if (fetchImpl === null) {
    global.fetch = undefined as unknown as typeof fetch;
  } else {
    global.fetch = fetchImpl as typeof fetch;
  }
  const href = `${modelsModuleHref}?t=${randomUUID()}`;
  return import(href);
}

function applyEnvOverride(key: EnvKey, value: string | number | null | undefined): void {
  if (value === undefined || value === null) {
    delete process.env[key];
  } else {
    process.env[key] = String(value);
  }
}

test("getAvailableModels merges remote data with fallbacks and caches calls", async () => {
  let callCount = 0;
  const mockFetch = async () => {
    callCount += 1;
    return {
      ok: true,
      status: 200,
      async json() {
        return {
          data: [{ id: "gpt-zeta" }, { id: "ft:skip-me" }, { id: "o4" }, { id: "deprecated-model" }]
        };
      }
    };
  };
  const { getAvailableModels } = await loadModelsModule({ apiKey: "token", fetchImpl: mockFetch });
  const first = await getAvailableModels();
  const second = await getAvailableModels();
  assert.equal(callCount, 1, "remote fetch should run only once due to caching");
  assert.deepEqual(first, second, "cached result should be reused");
  assert.ok(first.includes("gpt-zeta"), "new remote models should be included");
  assert.ok(first.includes("gpt-4o"), "fallback models should always be present");
  assert.ok(!first.includes("ft:skip-me"), "fine-tune models should be filtered out");
  assert.ok(!first.includes("deprecated-model"), "deprecated models should be filtered out");
});

test("updateModelSelection normalizes values and preserves manual effort when invalid input is ignored", async () => {
  const { updateModelSelection, getModelSettings } = await loadModelsModule({ fetchImpl: null });

  updateModelSelection({ model: "  custom-model  " });
  let settings = await getModelSettings();
  assert.equal(settings.model, "custom-model", "model names should be trimmed");

  updateModelSelection({ effort: "HIGH" });
  settings = await getModelSettings();
  assert.equal(settings.effort, "high", "effort should be normalized");

  updateModelSelection({ model: "", effort: "ultra" });
  settings = await getModelSettings();
  assert.equal(settings.model, null, "empty strings clear the manual model override");
  assert.equal(settings.effort, "high", "invalid effort leaves the previous value untouched");

  updateModelSelection({ model: null, effort: "" });
  settings = await getModelSettings();
  assert.equal(settings.model, null, "explicit null resets the manual model");
  assert.equal(settings.effort, null, "empty strings clear manual effort overrides");
  assert.ok(
    Array.isArray(settings.availableModels) && settings.availableModels.includes("gpt-4o"),
    "model settings should always expose at least the fallback models"
  );
});

test("getAvailableModels falls back to bundled list when remote fetch is unavailable", async () => {
  const { getAvailableModels } = await loadModelsModule({ apiKey: null, fetchImpl: null });
  const models = await getAvailableModels();
  const expected = Array.from(new Set(FALLBACK_MODELS)).sort((a, b) => a.localeCompare(b));
  assert.deepEqual(models, expected, "should return the bundled model list when fetch cannot run");
});

test("getAvailableModels coalesces concurrent fetches", async () => {
  let callCount = 0;
  const mockFetch = async () => {
    callCount += 1;
    await new Promise((resolve) => setTimeout(resolve, 10));
    return {
      ok: true,
      status: 200,
      async json() {
        return { data: [{ id: "gpt-concurrent" }] };
      }
    };
  };
  const { getAvailableModels } = await loadModelsModule({ apiKey: "token", fetchImpl: mockFetch });
  const [first, second] = await Promise.all([getAvailableModels(), getAvailableModels()]);
  assert.equal(callCount, 1, "inflight fetch should be shared across callers");
  assert.deepEqual(first, second, "concurrent callers should receive identical lists");
  assert.ok(
    first.includes("gpt-concurrent"),
    "remote models should still be included in the response"
  );
});
